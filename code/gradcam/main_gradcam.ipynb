{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broad-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import nilearn as nil\n",
    "from nilearn import image as nil_image\n",
    "import numpy as np\n",
    "\n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.autograd as dif\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.modules.utils import _triple\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# This is imported to fix any data error in a batch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Custom import\n",
    "from train_test_set import train_test_subs, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comparable-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmriDataset(Dataset):\n",
    "\n",
    "    def __init__(self, params, data_dir='/data/fmri/data', mask_path='/data/fmri/mask/caudate._mask.nii',\n",
    "                 img_shape=(57, 68, 49, 135), transform=None):\n",
    "        self.data_dir, self.params = data_dir, params\n",
    "        self.img_timesteps = params.img_timesteps\n",
    "        self.num_classes = params.nClass\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mask_path, self.img_shape = mask_path, img_shape\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        # Initialize the image indexes with their scores\n",
    "        self.index_data()\n",
    "        # self.mask = self.read_mask()\n",
    "        self.class_weights = self.find_weights()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path, score = self.samples[idx]\n",
    "            score = self.get_class(score)\n",
    "            img = self.read_image(img_path)\n",
    "            # img = self.apply_mask(img)\n",
    "            img = self.apply_temporal_aug(img)\n",
    "            return img, score\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def index_data(self):\n",
    "        \"\"\"\n",
    "        Stores all the image_paths with their respective scores/classes in the \n",
    "        \"\"\"\n",
    "        self.weights = {i: 0 for i in range(self.num_classes)}\n",
    "        for sub in os.listdir(self.data_dir):\n",
    "            if sub not in self.params.subs:\n",
    "                # Don't consider subjects that are not in the subs set\n",
    "                continue\n",
    "            sub_dir = os.path.join(self.data_dir, sub)\n",
    "            preproc_dir = os.path.join(sub_dir, f'{sub}.preproc')\n",
    "            for img_name in os.listdir(preproc_dir):\n",
    "                img_path = os.path.join(preproc_dir, img_name)\n",
    "                score = self.get_score(sub_dir, img_name)\n",
    "                score_class = self.get_class(score)\n",
    "                # Since we are randomly sampling 30 timesteps from each scan of 135 timesteps,\n",
    "                # I am considering the same image for \"n\" times so that we have more data to train\n",
    "                n = 5\n",
    "                for k in range(n):\n",
    "                    self.weights[score_class] += 1\n",
    "                    self.samples.append((img_path, score))\n",
    "\n",
    "    def get_class(self, score):\n",
    "        \"\"\"\n",
    "        Categorize each score into one of the five classes (bins)\n",
    "        Returns values from 0-5 (6 classes)\n",
    "        Classes: (0, 10), (10, 20), (20, 40), (40, 60), (60, 80), (80, 100)\n",
    "        \"\"\"\n",
    "        if score < 10:\n",
    "            return 0\n",
    "        elif score >= 10 and score < 20:\n",
    "            return 1\n",
    "        elif score >= 20 and score < 40:\n",
    "            return 2\n",
    "        elif score >= 40 and score < 60:\n",
    "            return 3\n",
    "        elif score >= 60 and score < 80:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "\n",
    "    def get_score(self, sub_dir, img_name):\n",
    "        score_file = '0back_VAS-f.1D' if '0back' in img_name else '2back_VAS-f.1D'\n",
    "        score_path = os.path.join(sub_dir, score_file)\n",
    "        with open(score_path, 'r') as s_f:\n",
    "            scores = [int(str(score.replace('\\n', ''))) for score in s_f]\n",
    "\n",
    "        task_num = img_name.split('.')[1]\n",
    "        score_num = int(task_num[-1:])\n",
    "        return scores[score_num]\n",
    "\n",
    "    def read_image(self, img_path):\n",
    "        try:\n",
    "            nX, nY, nZ, nT = self.img_shape\n",
    "            img = nil_image.load_img(img_path)\n",
    "            img = img.get_fdata()[:nX, :nY, :nZ, :nT]\n",
    "            img = torch.tensor(img, dtype=torch.float, device=self.device)\n",
    "            img = (img - img.mean()) / img.std()\n",
    "        except Exception as error:\n",
    "            print(img_path)\n",
    "        return img\n",
    "\n",
    "    def read_mask(self):\n",
    "        nX, nY, nZ, _ = self.img_shape\n",
    "        mask_img = nil_image.load_img(self.mask_path)\n",
    "        mask_img = mask_img.get_fdata()[:]\n",
    "        mask_img = np.asarray(mask_img)\n",
    "        dilated_mask = np.zeros((nX, nY, nZ))\n",
    "        ratio = round(mask_img.shape[2]/nZ)\n",
    "        for k in range(nZ):\n",
    "            temp = ndimage.morphology.binary_dilation(\n",
    "                mask_img[:, :, k*ratio], iterations=1) * 1\n",
    "            temp_img = Image.fromarray(np.uint8(temp*255))\n",
    "            dilated_mask[:, :, k] = np.array(temp_img.resize((nY, nX)))\n",
    "\n",
    "        dilated_mask = (dilated_mask > 64).astype(int)\n",
    "        dilated_mask = torch.tensor(\n",
    "            dilated_mask, dtype=torch.float, device=self.device)\n",
    "        return dilated_mask\n",
    "\n",
    "    def apply_mask(self, img):\n",
    "        nT = img.shape[-1]\n",
    "        for i in range(nT):\n",
    "            img[:, :, :, i] = torch.mul(img[:, :, :, i], self.mask)\n",
    "        return img\n",
    "\n",
    "    def apply_temporal_aug(self, img):\n",
    "        \"\"\"\n",
    "        Image shape: X, Y, Z, t=135\n",
    "        So, e.g: take any 30 random timesteps from the 135 available in ascending order \n",
    "        \"\"\"\n",
    "        total_timesteps = img.shape[3]\n",
    "        rand_timesteps = sorted(random.sample(\n",
    "            range(0, total_timesteps), self.img_timesteps))\n",
    "        # Move time axes to the first place followed by X, Y, Z\n",
    "        img = img.permute(3, 0, 1, 2)\n",
    "        img = torch.tensor(np.take(img.cpu().numpy(), rand_timesteps, axis=0))\n",
    "        return img\n",
    "\n",
    "    def find_weights(self):\n",
    "        weights = dict(self.weights)\n",
    "        key_max = max(weights.keys(), key=(lambda k: weights[k]))\n",
    "        max_value = weights[key_max]\n",
    "        for key in weights.keys():\n",
    "            # Add 1 to the denominator to avoid divide by zero error (in some cases)\n",
    "            weights[key] = max_value / (weights[key]+1)\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collective-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmriModel(nn.Module):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(FmriModel, self).__init__()\n",
    "\n",
    "        self.ndf = params.ndf\n",
    "        # \"nc\" is the number of timesteps in the input scan (t=nc in this case)\n",
    "        self.nc = params.img_timesteps\n",
    "        self.nClass = params.nClass\n",
    "\n",
    "        # Input to the model is (t, 57, 68, 49) <== (t, x, y, z)\n",
    "        # 't' can change based on the \"img_timesteps\" value (number of timesteps to be sampled from one scan)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(params.nX, self.ndf, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(self.ndf*1, self.ndf*2, 3, 2, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf*2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(self.ndf*2, self.ndf*4, 3, 2, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf*4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self._to_linear, self._to_lstm = None, None\n",
    "        x = torch.randn(params.batchSize, self.nc,\n",
    "                        params.nX, params.nY, params.nZ)\n",
    "        self.convs(x)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self._to_lstm, hidden_size=128,\n",
    "                            num_layers=1, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, self.nClass)\n",
    "\n",
    "#         self.fc2 = nn.Sequential(\n",
    "#             nn.Linear(self.ndf * 1, self.nClass),\n",
    "#         )\n",
    "\n",
    "    def convs(self, x):\n",
    "        batch_size, timesteps, c, h, w = x.size()\n",
    "        x = x.view(batch_size*timesteps, c, h, w)\n",
    "        x = self.conv1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.conv3(x)\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            # First pass: done to know what the output of the convnet is\n",
    "            self._to_linear = int(x[0].shape[0]*x[0].shape[1]*x[0].shape[2])\n",
    "            r_in = x.view(batch_size, timesteps, -1)\n",
    "            #print(r_in.shape)\n",
    "            self._to_lstm = r_in.shape[2]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, c, h, w = x.size()\n",
    "        cnn_out = self.convs(x)\n",
    "\n",
    "        # Prepare the output from CNN to pass through the LSTM layer\n",
    "        r_in = cnn_out.view(batch_size, timesteps, -1)\n",
    "\n",
    "        # Flattening is required when we use DataParallel\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        # Get output from the LSTM\n",
    "        r_out, (h_n, h_c) = self.lstm(r_in)\n",
    "\n",
    "        # Pass the output of the LSTM to FC layers\n",
    "        r_out = self.fc1(r_out[:, -1, :])\n",
    "        # r_out = self.fc2(r_out)\n",
    "\n",
    "        # Apply softmax to the output and return it\n",
    "        return F.log_softmax(r_out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "massive-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    # Function to catch errors while reading a batch of fMRI scans\n",
    "    # Removes any NoneType values from the batch to prevent errors while training\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return default_collate(batch)\n",
    "    \n",
    "\n",
    "def train_test_length(total, test_pct=0.2):\n",
    "    train_count = int((1-test_pct)*total)\n",
    "    test_count = total - train_count\n",
    "    return train_count, test_count\n",
    "\n",
    "\n",
    "def train(net, train_loader, loss_function, optimizer, test_loader):\n",
    "    print('Training...')\n",
    "    for epoch in range(params.nEpochs):\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Validating...')\n",
    "        _, _, train_acc = test(net, train_loader)\n",
    "        _, _, test_acc = test(net, test_loader)\n",
    "\n",
    "        print(f'Epoch: {epoch} | Loss: {loss} | Train Acc: {train_acc} | Test Acc: {test_acc}')\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def test(net, test_loader):\n",
    "    # print('Testing...')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    preds = []\n",
    "    actual = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            if not data:\n",
    "                continue\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            class_outputs = net(inputs)\n",
    "            _, class_prediction = torch.max(class_outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (class_prediction == labels).sum().item()\n",
    "            preds.extend(list(class_prediction.to(dtype=torch.int64)))\n",
    "            actual.extend(list(labels.to(dtype=torch.int64)))\n",
    "\n",
    "    acc = 100*correct/total\n",
    "    # print(f'Accuracy: {acc}')\n",
    "    return preds, actual, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thousand-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmriGradModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(FmriGradModel, self).__init__()\n",
    "        \n",
    "        # Get pretrained Fmri model\n",
    "        self.fmri_model = FmriModel(params=params)\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        if (device.type == 'cuda') and (n_gpus > 1):\n",
    "            self.fmri_model = nn.DataParallel(self.fmri_model, list(range(n_gpus)))\n",
    "            \n",
    "        self.fmri_model.load_state_dict(torch.load('/home/ashish/Documents/github/fmri-TBI/code/03_26_2021_01_22-fold-0-lr-0.0001.pth'))\n",
    "        \n",
    "        \n",
    "        # Placeholder for the gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def features_conv(self, x):\n",
    "        batch_size, timesteps, c, h, w = x.size()\n",
    "        # Merge batch_size and timesteps into one dimension\n",
    "        x = x.view(batch_size*timesteps, c, h, w)\n",
    "        \n",
    "        return self.fmri_model.module.conv1(x)\n",
    "    \n",
    "    def features_others(self, x):        \n",
    "        self.fmri_model.module.lstm.flatten_parameters()\n",
    "\n",
    "        # Get output from the LSTM\n",
    "        r_out, (h_n, h_c) = self.fmri_model.module.lstm(x)\n",
    "\n",
    "        # Pass the output of the LSTM to FC layers\n",
    "        r_out = self.fmri_model.module.fc1(r_out[:, -1, :])\n",
    "        # r_out = self.fmri_model.module.fc2(r_out)\n",
    "\n",
    "        # Apply softmax to the output and return it\n",
    "        return F.log_softmax(r_out, dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, c, h, w = x.size()\n",
    "        \n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        # Register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # Apply the remaining layers\n",
    "        x = x.view(batch_size, timesteps, -1)\n",
    "        return self.features_others(x)\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        # Method for gradient extraction\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "divine-turning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: LR: 0.0001 | Epochs: 10 | K-folds: 1 | BatchSize: 8 | Sample timesteps: 85\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "k_fold = 1\n",
    "params.nEpochs = 10\n",
    "params.batchSize = 8\n",
    "accs = []\n",
    "cf_matrix = []\n",
    "learning_rate = 0.0001\n",
    "sample_timesteps = 85\n",
    "params.update({'img_timesteps': sample_timesteps})\n",
    "\n",
    "print(f'Parameters: LR: {learning_rate} | Epochs: {params.nEpochs} | K-folds: {k_fold} | BatchSize: {params.batchSize} | Sample timesteps: {sample_timesteps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "threatened-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri = FmriGradModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greek-intermediate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subs: ['sub-hc001', 'sub-hc002', 'sub-hc004', 'sub-hc005', 'sub-hc009', 'sub-hc010', 'sub-hc016', 'sub-hc019', 'sub-hc020', 'sub-hc021', 'sub-hc022', 'sub-hc023', 'sub-hc024', 'sub-hc025', 'sub-hc028', 'sub-hc029', 'sub-hc030', 'sub-hc031', 'sub-hc033', 'sub-tbi001', 'sub-tbi002', 'sub-tbi003', 'sub-tbi004', 'sub-tbi008', 'sub-tbi009', 'sub-tbi010', 'sub-tbi011', 'sub-tbi012', 'sub-tbi013', 'sub-tbi014', 'sub-tbi015', 'sub-tbi016', 'sub-tbi018', 'sub-tbi019', 'sub-tbi020', 'sub-tbi023', 'sub-tbi025', 'sub-tbi027', 'sub-tbi029', 'sub-tbi030', 'sub-tbi034', 'sub-tbi035', 'sub-tbi036'] || Test subs: ['sub-hc011', 'sub-hc007', 'sub-hc003', 'sub-hc012', 'sub-tbi022', 'sub-tbi017', 'sub-tbi006', 'sub-tbi024', 'sub-tbi005']\n"
     ]
    }
   ],
   "source": [
    "# Load images to test for forward passes\n",
    "train_subs, test_subs = train_test_subs(test_pct=0.2)\n",
    "print(f'Train subs: {train_subs} || Test subs: {test_subs}')\n",
    "\n",
    "params.update({'subs': train_subs})\n",
    "trainset = FmriDataset(params=params)\n",
    "params.update({'subs': test_subs})\n",
    "testset = FmriDataset(params=params)\n",
    "\n",
    "class_weights = torch.FloatTensor(\n",
    "    [trainset.class_weights[i] for i in range(params.nClass)]).to(device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        trainset, batch_size=params.batchSize, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    testset, batch_size=params.batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unique-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, scores = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "likely-command",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 2, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "activated-nursing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([85, 57, 68, 49])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "architectural-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fmri(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "satellite-liver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3321e-02, -4.4762e+00, -5.1234e+00, -4.3246e+00, -5.0005e+00,\n",
       "         -5.2806e+00],\n",
       "        [-1.4220e-02, -6.1429e+00, -5.5370e+00, -5.4910e+00, -6.3354e+00,\n",
       "         -6.1488e+00],\n",
       "        [-5.3995e-02, -4.0846e+00, -4.4290e+00, -4.6955e+00, -5.6107e+00,\n",
       "         -4.5087e+00],\n",
       "        [-4.1666e-02, -4.1446e+00, -4.7686e+00, -4.6567e+00, -5.5235e+00,\n",
       "         -5.8164e+00],\n",
       "        [-4.4225e-02, -4.0745e+00, -4.7503e+00, -4.7340e+00, -5.1561e+00,\n",
       "         -5.7908e+00],\n",
       "        [-3.0491e-02, -4.3334e+00, -5.4303e+00, -4.9562e+00, -5.8138e+00,\n",
       "         -5.9910e+00],\n",
       "        [-7.1561e+00, -7.2176e+00, -4.3171e-03, -6.5380e+00, -7.8195e+00,\n",
       "         -6.9644e+00],\n",
       "        [-3.0503e+00, -7.4343e-02, -4.4960e+00, -5.3895e+00, -5.9051e+00,\n",
       "         -5.1394e+00]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "thirty-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0][0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "seasonal-alarm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([680, 8, 66, 47])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients = fmri.get_activations_gradient()\n",
    "gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "identified-length",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([680, 8, 66, 47])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = fmri.get_activations(imgs).detach()\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "atlantic-responsibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.653288201160541"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_shape = 57*68*49\n",
    "gradient_shape = 8*66*47\n",
    "original_shape/gradient_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-agreement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
