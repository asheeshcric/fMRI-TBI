{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import nilearn as nil\n",
    "from nilearn import image as nil_image\n",
    "import numpy as np\n",
    "\n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.autograd as dif\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.modules.utils import _triple\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# This is imported to fix any data error in a batch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Custom import\n",
    "from train_test_set import train_test_subs, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmriDataset(Dataset):\n",
    "\n",
    "    def __init__(self, params, data_dir='/data/fmri/data', mask_path='/data/fmri/mask/caudate._mask.nii',\n",
    "                 img_shape=(57, 68, 49, 135)):\n",
    "        self.data_dir, self.params = data_dir, params\n",
    "        self.img_timesteps = params.img_timesteps\n",
    "        self.num_classes = params.nClass\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mask_path, self.img_shape = mask_path, img_shape\n",
    "        self.samples = []\n",
    "        # Initialize the image indexes with their scores\n",
    "        self.index_data()\n",
    "        # self.mask = self.read_mask()\n",
    "        self.class_weights = self.find_weights()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path, score = self.samples[idx]\n",
    "            score = self.get_class(score)\n",
    "            img = self.read_image(img_path)\n",
    "            # img = self.apply_mask(img)\n",
    "            img = self.apply_temporal_aug(img)\n",
    "            return img, score\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def index_data(self):\n",
    "        \"\"\"\n",
    "        Stores all the image_paths with their respective scores/classes in the \n",
    "        \"\"\"\n",
    "        self.weights = {i: 0 for i in range(self.num_classes)}\n",
    "        for sub in os.listdir(self.data_dir):\n",
    "            if sub not in self.params.subs:\n",
    "                # Don't consider subjects that are not in the subs set\n",
    "                continue\n",
    "            sub_dir = os.path.join(self.data_dir, sub)\n",
    "            preproc_dir = os.path.join(sub_dir, f'{sub}.preproc')\n",
    "            for img_name in os.listdir(preproc_dir):\n",
    "                img_path = os.path.join(preproc_dir, img_name)\n",
    "                score = self.get_score(sub_dir, img_name)\n",
    "                score_class = self.get_class(score)\n",
    "                # Since we are randomly sampling 30 timesteps from each scan of 135 timesteps,\n",
    "                # I am considering the same image for \"n\" times so that we have more data to train\n",
    "                n = 5\n",
    "                for k in range(n):\n",
    "                    self.weights[score_class] += 1\n",
    "                    self.samples.append((img_path, score))\n",
    "\n",
    "    def get_class(self, score):\n",
    "        \"\"\"\n",
    "        Categorize each score into one of the five classes (bins)\n",
    "        Returns values from 0-5 (6 classes)\n",
    "        Classes: (0, 10), (10, 20), (20, 40), (40, 60), (60, 80), (80, 100)\n",
    "        \"\"\"\n",
    "        if score < 10:\n",
    "            return 0\n",
    "        elif score >= 10 and score < 20:\n",
    "            return 1\n",
    "        elif score >= 20 and score < 40:\n",
    "            return 2\n",
    "        elif score >= 40 and score < 60:\n",
    "            return 3\n",
    "        elif score >= 60 and score < 80:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "\n",
    "    def get_score(self, sub_dir, img_name):\n",
    "        score_file = '0back_VAS-f.1D' if '0back' in img_name else '2back_VAS-f.1D'\n",
    "        score_path = os.path.join(sub_dir, score_file)\n",
    "        with open(score_path, 'r') as s_f:\n",
    "            scores = [int(str(score.replace('\\n', ''))) for score in s_f]\n",
    "\n",
    "        task_num = img_name.split('.')[1]\n",
    "        score_num = int(task_num[-1:])\n",
    "        return scores[score_num]\n",
    "\n",
    "    def read_image(self, img_path):\n",
    "        nX, nY, nZ, nT = self.img_shape\n",
    "        img = nil_image.load_img(img_path)\n",
    "        img = img.get_fdata()[:nX, :nY, :nZ, :nT]\n",
    "        img = torch.tensor(img, dtype=torch.float, device=self.device)\n",
    "        img = (img - img.mean()) / img.std()\n",
    "        return img\n",
    "\n",
    "    def read_mask(self):\n",
    "        nX, nY, nZ, _ = self.img_shape\n",
    "        mask_img = nil.image.load_img(self.mask_path)\n",
    "        mask_img = mask_img.get_fdata()[:]\n",
    "        mask_img = np.asarray(mask_img)\n",
    "        dilated_mask = np.zeros((nX, nY, nZ))\n",
    "        ratio = round(mask_img.shape[2]/nZ)\n",
    "        for k in range(nZ):\n",
    "            temp = ndimage.morphology.binary_dilation(\n",
    "                mask_img[:, :, k*ratio], iterations=1) * 1\n",
    "            temp_img = Image.fromarray(np.uint8(temp*255))\n",
    "            dilated_mask[:, :, k] = np.array(temp_img.resize((nY, nX)))\n",
    "\n",
    "        dilated_mask = (dilated_mask > 64).astype(int)\n",
    "        dilated_mask = torch.tensor(\n",
    "            dilated_mask, dtype=torch.float, device=self.device)\n",
    "        return dilated_mask\n",
    "\n",
    "    def apply_mask(self, img):\n",
    "        nT = img.shape[-1]\n",
    "        for i in range(nT):\n",
    "            img[:, :, :, i] = torch.mul(img[:, :, :, i], self.mask)\n",
    "        return img\n",
    "\n",
    "    def apply_temporal_aug(self, img):\n",
    "        \"\"\"\n",
    "        Image shape: X, Y, Z, t=135\n",
    "        So, e.g: take any 30 random timesteps from the 135 available in ascending order \n",
    "        \"\"\"\n",
    "        total_timesteps = img.shape[3]\n",
    "        rand_timesteps = sorted(random.sample(\n",
    "            range(0, total_timesteps), self.img_timesteps))\n",
    "        # Move time axes to the first place followed by X, Y, Z\n",
    "        img = img.permute(3, 0, 1, 2)\n",
    "        img = torch.tensor(np.take(img.cpu().numpy(), rand_timesteps, axis=0))\n",
    "        return img\n",
    "\n",
    "    def find_weights(self):\n",
    "        weights = dict(self.weights)\n",
    "        key_max = max(weights.keys(), key=(lambda k: weights[k]))\n",
    "        max_value = weights[key_max]\n",
    "        for key in weights.keys():\n",
    "            # Add 1 to the denominator to avoid divide by zero error (in some cases)\n",
    "            weights[key] = max_value / (weights[key]+1)\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmriModel(nn.Module):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(FmriModel, self).__init__()\n",
    "\n",
    "        self.ndf = params.ndf\n",
    "        # \"nc\" is the number of timesteps in the input scan (t=nc in this case)\n",
    "        self.nc = params.img_timesteps\n",
    "        self.nClass = params.nClass\n",
    "\n",
    "        # Input to the model is (t, 57, 68, 49) <== (t, x, y, z)\n",
    "        # 't' can change based on the \"img_timesteps\" value (number of timesteps to be sampled from one scan)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(params.nX, self.ndf, 5, 2, bias=False),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(self.ndf*1, self.ndf*2, 5, 2, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf*2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(self.ndf*2, self.ndf*4, 5, 2, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf*4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self._to_linear, self._to_lstm = None, None\n",
    "        x = torch.randn(params.batchSize*self.nc,\n",
    "                        params.nX, params.nY, params.nZ)\n",
    "        self.convs(x)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=3840, hidden_size=256,\n",
    "                            num_layers=1, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(256, self.ndf * 1)\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(self.ndf * 1, self.nClass),\n",
    "        )\n",
    "\n",
    "    def convs(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            # First pass: done to know what the output of the convnet is\n",
    "            self._to_linear = int(x[0].shape[0]*x[0].shape[1]*x[0].shape[2])\n",
    "            # For LSTM input, divide by batch_size and time_steps (i.e. / by self.nc and 1)\n",
    "            self._to_lstm = int(self._to_linear/self.nc)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, c, h, w = x.size()\n",
    "        # Merge batch_size and timesteps into one dimension\n",
    "        x = x.view(batch_size*timesteps, c, h, w)\n",
    "        cnn_out = self.convs(x)\n",
    "\n",
    "        # Prepare the output from CNN to pass through the LSTM layer\n",
    "        r_in = cnn_out.view(batch_size, timesteps, -1)\n",
    "\n",
    "        # Flattening is required when we use DataParallel\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        # Get output from the LSTM\n",
    "        r_out, (h_n, h_c) = self.lstm(r_in)\n",
    "\n",
    "        # Pass the output of the LSTM to FC layers\n",
    "        r_out = self.fc1(r_out[:, -1, :])\n",
    "        r_out = self.fc2(r_out)\n",
    "\n",
    "        # Apply softmax to the output and return it\n",
    "        return F.log_softmax(r_out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    # Function to catch errors while reading a batch of fMRI scans\n",
    "    # Removes any NoneType values from the batch to prevent errors while training\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return default_collate(batch)\n",
    "    \n",
    "\n",
    "def train_test_length(total, test_pct=0.2):\n",
    "    train_count = int((1-test_pct)*total)\n",
    "    test_count = total - train_count\n",
    "    return train_count, test_count\n",
    "\n",
    "\n",
    "def train(net, train_loader, loss_function, optimizer, test_loader):\n",
    "    print('Training...')\n",
    "    for epoch in range(params.nEpochs):\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        _, _, train_acc = test(net, train_loader)\n",
    "        _, _, test_acc = test(net, test_loader)\n",
    "\n",
    "        print(f'Epoch: {epoch} | Loss: {loss} | Train Acc: {train_acc} | Test Acc: {test_acc}')\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def test(net, test_loader):\n",
    "    # print('Testing...')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    preds = []\n",
    "    actual = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            if not data:\n",
    "                continue\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            class_outputs = net(inputs)\n",
    "            _, class_prediction = torch.max(class_outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (class_prediction == labels).sum().item()\n",
    "            preds.extend(list(class_prediction.to(dtype=torch.int64)))\n",
    "            actual.extend(list(labels.to(dtype=torch.int64)))\n",
    "\n",
    "    acc = 100*correct/total\n",
    "    # print(f'Accuracy: {acc}')\n",
    "    return preds, actual, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: LR: 0.0001 | Epochs: 10 | K-folds: 1 | BatchSize: 8 | Sample timesteps: 100\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "k_fold = 1\n",
    "params.nEpochs = 10\n",
    "params.batchSize = 8\n",
    "accs = []\n",
    "cf_matrix = []\n",
    "learning_rate = 0.0001\n",
    "sample_timesteps = 100\n",
    "params.update({'img_timesteps': sample_timesteps})\n",
    "\n",
    "print(f'Parameters: LR: {learning_rate} | Epochs: {params.nEpochs} | K-folds: {k_fold} | BatchSize: {params.batchSize} | Sample timesteps: {sample_timesteps}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmriGradModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(FmriGradModel, self).__init__()\n",
    "        \n",
    "        # Get pretrained Fmri model\n",
    "        self.fmri_model = FmriModel(params=params)\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        if (device.type == 'cuda') and (n_gpus > 1):\n",
    "            self.fmri_model = nn.DataParallel(self.fmri_model, list(range(n_gpus)))\n",
    "            \n",
    "        self.fmri_model.load_state_dict(torch.load('/home/ashish/Documents/github/fmri-TBI/code/01_03_2021_10_23-fold-0-lr-0.0001.pth'))\n",
    "        \n",
    "        \n",
    "        # Placeholder for the gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def features_conv(self, x):\n",
    "        x = self.fmri_model.module.conv1(x)\n",
    "        x = self.fmri_model.module.conv2(x)\n",
    "        return self.fmri_model.module.conv3(x)\n",
    "    \n",
    "    def features_others(self, x):        \n",
    "        self.fmri_model.module.lstm.flatten_parameters()\n",
    "\n",
    "        # Get output from the LSTM\n",
    "        r_out, (h_n, h_c) = self.fmri_model.module.lstm(x)\n",
    "\n",
    "        # Pass the output of the LSTM to FC layers\n",
    "        r_out = self.fmri_model.module.fc1(r_out[:, -1, :])\n",
    "        r_out = self.fmri_model.module.fc2(r_out)\n",
    "\n",
    "        # Apply softmax to the output and return it\n",
    "        return F.log_softmax(r_out, dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, c, h, w = x.size()\n",
    "        # Merge batch_size and timesteps into one dimension\n",
    "        x = x.view(batch_size*timesteps, c, h, w)\n",
    "        \n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        # Register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # Apply the remaining layers\n",
    "        return self.features_others(x)\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        # Method for gradient extraction\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.features_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri = FmriGradModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FmriGradModel(\n",
       "  (fmri_model): DataParallel(\n",
       "    (module): FmriModel(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(57, 64, kernel_size=(5, 5), stride=(2, 2), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (lstm): LSTM(3840, 256, batch_first=True)\n",
       "      (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (fc2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=6, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set to evaluation mode\n",
    "fmri.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subs: ['sub-hc024', 'sub-hc002', 'sub-hc007', 'sub-hc030', 'sub-hc010', 'sub-hc004', 'sub-hc009', 'sub-hc001', 'sub-hc012', 'sub-hc019', 'sub-hc011', 'sub-hc003', 'sub-hc025', 'sub-hc021', 'sub-hc028', 'sub-hc034', 'sub-hc005', 'sub-hc023', 'sub-hc029', 'sub-hc016', 'sub-hc031', 'sub-tbi019', 'sub-tbi018', 'sub-tbi020', 'sub-tbi024', 'sub-tbi015', 'sub-tbi013', 'sub-tbi011', 'sub-tbi034', 'sub-tbi006', 'sub-tbi027', 'sub-tbi009', 'sub-tbi017', 'sub-tbi025', 'sub-tbi002', 'sub-tbi022', 'sub-tbi004', 'sub-tbi030', 'sub-tbi036', 'sub-tbi035', 'sub-tbi014', 'sub-tbi001', 'sub-tbi010'] || Test subs: ['sub-hc033', 'sub-hc020', 'sub-hc022', 'sub-tbi003', 'sub-tbi005', 'sub-tbi008', 'sub-tbi012', 'sub-tbi016', 'sub-tbi023', 'sub-tbi029']\n"
     ]
    }
   ],
   "source": [
    "# Load images to test for forward passes\n",
    "train_subs, test_subs = train_test_subs(test_pct=0.2)\n",
    "print(f'Train subs: {train_subs} || Test subs: {test_subs}')\n",
    "\n",
    "params.update({'subs': train_subs})\n",
    "trainset = FmriDataset(params=params)\n",
    "params.update({'subs': test_subs})\n",
    "testset = FmriDataset(params=params)\n",
    "\n",
    "class_weights = torch.FloatTensor(\n",
    "    [trainset.class_weights[i] for i in range(params.nClass)]).to(device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        trainset, batch_size=params.batchSize, shuffle=True, collate_fn=my_collate)\n",
    "test_loader = DataLoader(\n",
    "    testset, batch_size=params.batchSize, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, score = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 1, 3, 0, 4, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-aa474dfc026f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tankpool/home/ashish/.virtualenvs/fmri/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b92d49b19319>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Apply the remaining layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_others\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_activations_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b92d49b19319>\u001b[0m in \u001b[0;36mfeatures_others\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Get output from the LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmri_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Pass the output of the LSTM to FC layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tankpool/home/ashish/.virtualenvs/fmri/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tankpool/home/ashish/.virtualenvs/fmri/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m/tankpool/home/ashish/.virtualenvs/fmri/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[Tensor, Tensor], Optional[Tensor]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tankpool/home/ashish/.virtualenvs/fmri/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    164\u001b[0m             raise RuntimeError(\n\u001b[1;32m    165\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m--> 166\u001b[0;31m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 4"
     ]
    }
   ],
   "source": [
    "preds = fmri(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in range(k_fold):\n",
    "    train_subs, test_subs = train_test_subs(test_pct=0.2)\n",
    "    print(f'Train subs: {train_subs} || Test subs: {test_subs}')\n",
    "    # print(f'Train-subs: {len(train_subs)}')\n",
    "    # print(f'Test-subs: {len(test_subs)}')\n",
    "    params.update({'subs': train_subs})\n",
    "    trainset = FmriDataset(params=params)\n",
    "    params.update({'subs': test_subs})\n",
    "    testset = FmriDataset(params=params)\n",
    "\n",
    "    class_weights = torch.FloatTensor(\n",
    "        [trainset.class_weights[i] for i in range(params.nClass)]).to(device)\n",
    "    # Initialize the model\n",
    "    net = FmriModel(params=params).to(device)\n",
    "    # Distributed training on multiple GPUs if available\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f'Number of GPUs available: {n_gpus}')\n",
    "    if (device.type == 'cuda') and (n_gpus > 1):\n",
    "        net = nn.DataParallel(net, list(range(n_gpus)))\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        trainset, batch_size=params.batchSize, shuffle=True, collate_fn=my_collate)\n",
    "    test_loader = DataLoader(\n",
    "        testset, batch_size=params.batchSize, shuffle=True, collate_fn=my_collate)\n",
    "    \n",
    "    # You can use my_collate() function inside the dataloader to check for errors while reading corrupted scans\n",
    "\n",
    "    net = train(net, train_loader, loss_function, optimizer, test_loader)\n",
    "    # Save the model checkpoint\n",
    "    current_time = datetime.now()\n",
    "    current_time = current_time.strftime(\"%m_%d_%Y_%H_%M\")\n",
    "    torch.save(net.state_dict(), f'{current_time}-fold-{k}-lr-{learning_rate}.pth')\n",
    "    preds, actual, acc = test(net, test_loader)\n",
    "    accs.append(acc)\n",
    "\n",
    "    # For confusion matrix\n",
    "    preds = [int(k) for k in preds]\n",
    "    actual = [int(k) for k in actual]\n",
    "\n",
    "    cf = confusion_matrix(actual, preds, labels=list(range(params.nClass)))\n",
    "    cf_matrix.append(cf)\n",
    "    with open('cf_matrices.txt', 'a') as cf_file:\n",
    "        cf_file.write(str(cf) + '\\n')\n",
    "\n",
    "\n",
    "print(cf_matrix)\n",
    "print(accs)\n",
    "print(f'Avg Accuracy: {sum(accs)/len(accs)}')\n",
    "print(f'Parameters: LR: {learning_rate} | Epochs: {params.nEpochs} | K-folds: {k_fold} | BatchSize: {params.batchSize} | Sample timesteps: {sample_timesteps}')\n",
    "\n",
    "print(f'Train subs: {train_subs} || Test subs: {test_subs}')\n",
    "\n",
    "with open('abc.txt', 'w') as abc_file:\n",
    "    for cf in cf_matrix:\n",
    "        abc_file.write(f'{cf}\\n')\n",
    "    abc_file.write(f'{accs}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
