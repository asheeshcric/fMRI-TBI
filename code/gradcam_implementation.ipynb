{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import nilearn as nil\n",
    "from nilearn import image as nil_image\n",
    "import numpy as np\n",
    "\n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.autograd as dif\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.modules.utils import _triple\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# This is imported to fix any data error in a batch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Custom import\n",
    "from train_test_set import train_test_subs, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmriDataset(Dataset):\n",
    "\n",
    "    def __init__(self, params, data_dir='/data/fmri/data', mask_path='/data/fmri/mask/caudate._mask.nii',\n",
    "                 img_shape=(57, 68, 49, 135)):\n",
    "        self.data_dir, self.params = data_dir, params\n",
    "        self.img_timesteps = params.img_timesteps\n",
    "        self.num_classes = params.nClass\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mask_path, self.img_shape = mask_path, img_shape\n",
    "        self.samples = []\n",
    "        # Initialize the image indexes with their scores\n",
    "        self.index_data()\n",
    "        # self.mask = self.read_mask()\n",
    "        self.class_weights = self.find_weights()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path, score = self.samples[idx]\n",
    "            score = self.get_class(score)\n",
    "            img = self.read_image(img_path)\n",
    "            # img = self.apply_mask(img)\n",
    "            img = self.apply_temporal_aug(img)\n",
    "            return img, score\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def index_data(self):\n",
    "        \"\"\"\n",
    "        Stores all the image_paths with their respective scores/classes in the \n",
    "        \"\"\"\n",
    "        self.weights = {i: 0 for i in range(self.num_classes)}\n",
    "        for sub in os.listdir(self.data_dir):\n",
    "            if sub not in self.params.subs:\n",
    "                # Don't consider subjects that are not in the subs set\n",
    "                continue\n",
    "            sub_dir = os.path.join(self.data_dir, sub)\n",
    "            preproc_dir = os.path.join(sub_dir, f'{sub}.preproc')\n",
    "            for img_name in os.listdir(preproc_dir):\n",
    "                img_path = os.path.join(preproc_dir, img_name)\n",
    "                score = self.get_score(sub_dir, img_name)\n",
    "                score_class = self.get_class(score)\n",
    "                # Since we are randomly sampling 30 timesteps from each scan of 135 timesteps,\n",
    "                # I am considering the same image for \"n\" times so that we have more data to train\n",
    "                n = 5\n",
    "                for k in range(n):\n",
    "                    self.weights[score_class] += 1\n",
    "                    self.samples.append((img_path, score))\n",
    "\n",
    "    def get_class(self, score):\n",
    "        \"\"\"\n",
    "        Categorize each score into one of the five classes (bins)\n",
    "        Returns values from 0-5 (6 classes)\n",
    "        Classes: (0, 10), (10, 20), (20, 40), (40, 60), (60, 80), (80, 100)\n",
    "        \"\"\"\n",
    "        if score < 10:\n",
    "            return 0\n",
    "        elif score >= 10 and score < 20:\n",
    "            return 1\n",
    "        elif score >= 20 and score < 40:\n",
    "            return 2\n",
    "        elif score >= 40 and score < 60:\n",
    "            return 3\n",
    "        elif score >= 60 and score < 80:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "\n",
    "    def get_score(self, sub_dir, img_name):\n",
    "        score_file = '0back_VAS-f.1D' if '0back' in img_name else '2back_VAS-f.1D'\n",
    "        score_path = os.path.join(sub_dir, score_file)\n",
    "        with open(score_path, 'r') as s_f:\n",
    "            scores = [int(str(score.replace('\\n', ''))) for score in s_f]\n",
    "\n",
    "        task_num = img_name.split('.')[1]\n",
    "        score_num = int(task_num[-1:])\n",
    "        return scores[score_num]\n",
    "\n",
    "    def read_image(self, img_path):\n",
    "        nX, nY, nZ, nT = self.img_shape\n",
    "        img = nil_image.load_img(img_path)\n",
    "        img = img.get_fdata()[:nX, :nY, :nZ, :nT]\n",
    "        img = torch.tensor(img, dtype=torch.float, device=self.device)\n",
    "        img = (img - img.mean()) / img.std()\n",
    "        return img\n",
    "\n",
    "    def read_mask(self):\n",
    "        nX, nY, nZ, _ = self.img_shape\n",
    "        mask_img = nil.image.load_img(self.mask_path)\n",
    "        mask_img = mask_img.get_fdata()[:]\n",
    "        mask_img = np.asarray(mask_img)\n",
    "        dilated_mask = np.zeros((nX, nY, nZ))\n",
    "        ratio = round(mask_img.shape[2]/nZ)\n",
    "        for k in range(nZ):\n",
    "            temp = ndimage.morphology.binary_dilation(\n",
    "                mask_img[:, :, k*ratio], iterations=1) * 1\n",
    "            temp_img = Image.fromarray(np.uint8(temp*255))\n",
    "            dilated_mask[:, :, k] = np.array(temp_img.resize((nY, nX)))\n",
    "\n",
    "        dilated_mask = (dilated_mask > 64).astype(int)\n",
    "        dilated_mask = torch.tensor(\n",
    "            dilated_mask, dtype=torch.float, device=self.device)\n",
    "        return dilated_mask\n",
    "\n",
    "    def apply_mask(self, img):\n",
    "        nT = img.shape[-1]\n",
    "        for i in range(nT):\n",
    "            img[:, :, :, i] = torch.mul(img[:, :, :, i], self.mask)\n",
    "        return img\n",
    "\n",
    "    def apply_temporal_aug(self, img):\n",
    "        \"\"\"\n",
    "        Image shape: X, Y, Z, t=135\n",
    "        So, e.g: take any 30 random timesteps from the 135 available in ascending order \n",
    "        \"\"\"\n",
    "        total_timesteps = img.shape[3]\n",
    "        rand_timesteps = sorted(random.sample(\n",
    "            range(0, total_timesteps), self.img_timesteps))\n",
    "        # Move time axes to the first place followed by X, Y, Z\n",
    "        img = img.permute(3, 0, 1, 2)\n",
    "        img = torch.tensor(np.take(img.cpu().numpy(), rand_timesteps, axis=0))\n",
    "        return img\n",
    "\n",
    "    def find_weights(self):\n",
    "        weights = dict(self.weights)\n",
    "        key_max = max(weights.keys(), key=(lambda k: weights[k]))\n",
    "        max_value = weights[key_max]\n",
    "        for key in weights.keys():\n",
    "            # Add 1 to the denominator to avoid divide by zero error (in some cases)\n",
    "            weights[key] = max_value / (weights[key]+1)\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmriModel(nn.Module):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(FmriModel, self).__init__()\n",
    "\n",
    "        self.ndf = params.ndf\n",
    "        # \"nc\" is the number of timesteps in the input scan (t=nc in this case)\n",
    "        self.nc = params.img_timesteps\n",
    "        self.nClass = params.nClass\n",
    "\n",
    "        # Input to the model is (t, 57, 68, 49) <== (t, x, y, z)\n",
    "        # 't' can change based on the \"img_timesteps\" value (number of timesteps to be sampled from one scan)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(params.nX, self.ndf, 5, 2, bias=False),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(self.ndf*1, self.ndf*2, 5, 2, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf*2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(self.ndf*2, self.ndf*4, 5, 2, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf*4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self._to_linear, self._to_lstm = None, None\n",
    "        x = torch.randn(params.batchSize*self.nc,\n",
    "                        params.nX, params.nY, params.nZ)\n",
    "        self.convs(x)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=3840, hidden_size=256,\n",
    "                            num_layers=1, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(256, self.ndf * 1)\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(self.ndf * 1, self.nClass),\n",
    "        )\n",
    "\n",
    "    def convs(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            # First pass: done to know what the output of the convnet is\n",
    "            self._to_linear = int(x[0].shape[0]*x[0].shape[1]*x[0].shape[2])\n",
    "            # For LSTM input, divide by batch_size and time_steps (i.e. / by self.nc and 1)\n",
    "            self._to_lstm = int(self._to_linear/self.nc)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, c, h, w = x.size()\n",
    "        # Merge batch_size and timesteps into one dimension\n",
    "        x = x.view(batch_size*timesteps, c, h, w)\n",
    "        cnn_out = self.convs(x)\n",
    "\n",
    "        # Prepare the output from CNN to pass through the LSTM layer\n",
    "        r_in = cnn_out.view(batch_size, timesteps, -1)\n",
    "\n",
    "        # Flattening is required when we use DataParallel\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        # Get output from the LSTM\n",
    "        r_out, (h_n, h_c) = self.lstm(r_in)\n",
    "\n",
    "        # Pass the output of the LSTM to FC layers\n",
    "        r_out = self.fc1(r_out[:, -1, :])\n",
    "        r_out = self.fc2(r_out)\n",
    "\n",
    "        # Apply softmax to the output and return it\n",
    "        return F.log_softmax(r_out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    # Function to catch errors while reading a batch of fMRI scans\n",
    "    # Removes any NoneType values from the batch to prevent errors while training\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return default_collate(batch)\n",
    "    \n",
    "\n",
    "def train_test_length(total, test_pct=0.2):\n",
    "    train_count = int((1-test_pct)*total)\n",
    "    test_count = total - train_count\n",
    "    return train_count, test_count\n",
    "\n",
    "\n",
    "def train(net, train_loader, loss_function, optimizer, test_loader):\n",
    "    print('Training...')\n",
    "    for epoch in range(params.nEpochs):\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        _, _, train_acc = test(net, train_loader)\n",
    "        _, _, test_acc = test(net, test_loader)\n",
    "\n",
    "        print(f'Epoch: {epoch} | Loss: {loss} | Train Acc: {train_acc} | Test Acc: {test_acc}')\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def test(net, test_loader):\n",
    "    # print('Testing...')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    preds = []\n",
    "    actual = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            if not data:\n",
    "                continue\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            class_outputs = net(inputs)\n",
    "            _, class_prediction = torch.max(class_outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (class_prediction == labels).sum().item()\n",
    "            preds.extend(list(class_prediction.to(dtype=torch.int64)))\n",
    "            actual.extend(list(labels.to(dtype=torch.int64)))\n",
    "\n",
    "    acc = 100*correct/total\n",
    "    # print(f'Accuracy: {acc}')\n",
    "    return preds, actual, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: LR: 0.0001 | Epochs: 10 | K-folds: 1 | BatchSize: 8 | Sample timesteps: 100\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "k_fold = 1\n",
    "params.nEpochs = 10\n",
    "params.batchSize = 8\n",
    "accs = []\n",
    "cf_matrix = []\n",
    "learning_rate = 0.0001\n",
    "sample_timesteps = 100\n",
    "params.update({'img_timesteps': sample_timesteps})\n",
    "\n",
    "print(f'Parameters: LR: {learning_rate} | Epochs: {params.nEpochs} | K-folds: {k_fold} | BatchSize: {params.batchSize} | Sample timesteps: {sample_timesteps}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmriGradModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(FmriGradModel, self).__init__()\n",
    "        \n",
    "        # Get pretrained Fmri model\n",
    "        self.fmri_model = FmriModel(params=params)\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        if (device.type == 'cuda') and (n_gpus > 1):\n",
    "            self.fmri_model = nn.DataParallel(self.fmri_model, list(range(n_gpus)))\n",
    "            \n",
    "        self.fmri_model.load_state_dict(torch.load('/home/ashish/Documents/github/fmri-TBI/code/01_03_2021_10_23-fold-0-lr-0.0001.pth'))\n",
    "        \n",
    "        \n",
    "        # Placeholder for the gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def features_conv(self, x):\n",
    "        batch_size, timesteps, c, h, w = x.size()\n",
    "        # Merge batch_size and timesteps into one dimension\n",
    "        x = x.view(batch_size*timesteps, c, h, w)\n",
    "        \n",
    "        x = self.fmri_model.module.conv1(x)\n",
    "        x = self.fmri_model.module.conv2(x)\n",
    "        return self.fmri_model.module.conv3(x)\n",
    "    \n",
    "    def features_others(self, x):        \n",
    "        self.fmri_model.module.lstm.flatten_parameters()\n",
    "\n",
    "        # Get output from the LSTM\n",
    "        r_out, (h_n, h_c) = self.fmri_model.module.lstm(x)\n",
    "\n",
    "        # Pass the output of the LSTM to FC layers\n",
    "        r_out = self.fmri_model.module.fc1(r_out[:, -1, :])\n",
    "        r_out = self.fmri_model.module.fc2(r_out)\n",
    "\n",
    "        # Apply softmax to the output and return it\n",
    "        return F.log_softmax(r_out, dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, c, h, w = x.size()\n",
    "        \n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        # Register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # Apply the remaining layers\n",
    "        x = x.view(batch_size, timesteps, -1)\n",
    "        return self.features_others(x)\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        # Method for gradient extraction\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri = FmriGradModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FmriGradModel(\n",
       "  (fmri_model): DataParallel(\n",
       "    (module): FmriModel(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(57, 64, kernel_size=(5, 5), stride=(2, 2), bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (lstm): LSTM(3840, 256, batch_first=True)\n",
       "      (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (fc2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=6, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set to evaluation mode\n",
    "fmri.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subs: ['sub-hc030', 'sub-hc034', 'sub-hc004', 'sub-hc001', 'sub-hc011', 'sub-hc021', 'sub-hc003', 'sub-hc009', 'sub-hc029', 'sub-hc007', 'sub-hc012', 'sub-hc019', 'sub-hc005', 'sub-hc028', 'sub-hc024', 'sub-hc031', 'sub-hc025', 'sub-hc010', 'sub-hc002', 'sub-hc022', 'sub-hc016', 'sub-tbi013', 'sub-tbi036', 'sub-tbi001', 'sub-tbi025', 'sub-tbi004', 'sub-tbi017', 'sub-tbi035', 'sub-tbi016', 'sub-tbi023', 'sub-tbi005', 'sub-tbi008', 'sub-tbi010', 'sub-tbi027', 'sub-tbi018', 'sub-tbi006', 'sub-tbi012', 'sub-tbi034', 'sub-tbi015', 'sub-tbi009', 'sub-tbi019', 'sub-tbi022', 'sub-tbi024'] || Test subs: ['sub-hc033', 'sub-hc020', 'sub-hc023', 'sub-tbi002', 'sub-tbi003', 'sub-tbi011', 'sub-tbi014', 'sub-tbi020', 'sub-tbi029', 'sub-tbi030']\n"
     ]
    }
   ],
   "source": [
    "# Load images to test for forward passes\n",
    "train_subs, test_subs = train_test_subs(test_pct=0.2)\n",
    "print(f'Train subs: {train_subs} || Test subs: {test_subs}')\n",
    "\n",
    "params.update({'subs': train_subs})\n",
    "trainset = FmriDataset(params=params)\n",
    "params.update({'subs': test_subs})\n",
    "testset = FmriDataset(params=params)\n",
    "\n",
    "class_weights = torch.FloatTensor(\n",
    "    [trainset.class_weights[i] for i in range(params.nClass)]).to(device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        trainset, batch_size=params.batchSize, shuffle=True, collate_fn=my_collate)\n",
    "test_loader = DataLoader(\n",
    "    testset, batch_size=params.batchSize, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, scores = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 5, 2, 2, 5, 0, 5, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fmri(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, class_prediction = torch.max(preds.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 5, 5, 2, 5, 0, 5, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6131e-03, -7.5353e+00, -6.6624e+00, -9.9262e+00, -7.3848e+00,\n",
       "         -8.9623e+00],\n",
       "        [-9.5114e+00, -7.8405e+00, -8.5160e+00, -6.0449e+00, -7.2433e+00,\n",
       "         -3.7597e-03],\n",
       "        [-3.3204e+00, -5.8140e+00, -7.7978e+00, -3.1478e+00, -2.9630e+00,\n",
       "         -1.4404e-01],\n",
       "        [-5.3396e+00, -4.6615e+00, -1.5729e-02, -8.8961e+00, -6.8593e+00,\n",
       "         -8.6857e+00],\n",
       "        [-6.2822e+00, -7.4924e+00, -4.2323e+00, -6.0478e+00, -6.1292e+00,\n",
       "         -2.1722e-02],\n",
       "        [-9.1174e-03, -5.7846e+00, -6.4308e+00, -5.5346e+00, -8.0160e+00,\n",
       "         -9.0929e+00],\n",
       "        [-8.2400e+00, -9.5053e+00, -1.0360e+01, -6.7412e+00, -6.4233e+00,\n",
       "         -3.1795e-03],\n",
       "        [-8.2506e+00, -7.0856e+00, -7.9803e+00, -9.3564e+00, -1.8089e-03,\n",
       "         -8.1787e+00]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0][0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 256, 5, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients = fmri.get_activations_gradient()\n",
    "gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "pooled_gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the activations of the last convolutional layer\n",
    "activations = fmri.get_activations(imgs).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 256, 5, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight the channels by corresponding gradients\n",
    "for i in range(256):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 5, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu on top of the heatmap\n",
    "heatmap = np.maximum(heatmap, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 5, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap.squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following plot is a single channel of size 5x3\n",
    "- Need to interpolate it to the original size of the input 67x49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f29fc362a20>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(heatmap.squeeze()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 68, 49, 135)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the corresponding image\n",
    "scan = nil_image.load_img('/data/fmri/data/sub-hc030/sub-hc030.preproc/0back.r01.scale.nii.gz')\n",
    "scan = scan.get_fdata()\n",
    "scan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 68, 49)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = scan[:, :, :, 0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f28f2095dd8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x399.673 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(np.mean(img, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected Ptr<cv::UMat> for argument 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-815401d174d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mheatmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyColorMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLORMAP_JET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument 'src'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[2]))\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.4 + np.mean(img, axis=0)\n",
    "plt.matshow(superimposed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in range(k_fold):\n",
    "    train_subs, test_subs = train_test_subs(test_pct=0.2)\n",
    "    print(f'Train subs: {train_subs} || Test subs: {test_subs}')\n",
    "    # print(f'Train-subs: {len(train_subs)}')\n",
    "    # print(f'Test-subs: {len(test_subs)}')\n",
    "    params.update({'subs': train_subs})\n",
    "    trainset = FmriDataset(params=params)\n",
    "    params.update({'subs': test_subs})\n",
    "    testset = FmriDataset(params=params)\n",
    "\n",
    "    class_weights = torch.FloatTensor(\n",
    "        [trainset.class_weights[i] for i in range(params.nClass)]).to(device)\n",
    "    # Initialize the model\n",
    "    net = FmriModel(params=params).to(device)\n",
    "    # Distributed training on multiple GPUs if available\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f'Number of GPUs available: {n_gpus}')\n",
    "    if (device.type == 'cuda') and (n_gpus > 1):\n",
    "        net = nn.DataParallel(net, list(range(n_gpus)))\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        trainset, batch_size=params.batchSize, shuffle=True, collate_fn=my_collate)\n",
    "    test_loader = DataLoader(\n",
    "        testset, batch_size=params.batchSize, shuffle=True, collate_fn=my_collate)\n",
    "    \n",
    "    # You can use my_collate() function inside the dataloader to check for errors while reading corrupted scans\n",
    "\n",
    "    net = train(net, train_loader, loss_function, optimizer, test_loader)\n",
    "    # Save the model checkpoint\n",
    "    current_time = datetime.now()\n",
    "    current_time = current_time.strftime(\"%m_%d_%Y_%H_%M\")\n",
    "    torch.save(net.state_dict(), f'{current_time}-fold-{k}-lr-{learning_rate}.pth')\n",
    "    preds, actual, acc = test(net, test_loader)\n",
    "    accs.append(acc)\n",
    "\n",
    "    # For confusion matrix\n",
    "    preds = [int(k) for k in preds]\n",
    "    actual = [int(k) for k in actual]\n",
    "\n",
    "    cf = confusion_matrix(actual, preds, labels=list(range(params.nClass)))\n",
    "    cf_matrix.append(cf)\n",
    "    with open('cf_matrices.txt', 'a') as cf_file:\n",
    "        cf_file.write(str(cf) + '\\n')\n",
    "\n",
    "\n",
    "print(cf_matrix)\n",
    "print(accs)\n",
    "print(f'Avg Accuracy: {sum(accs)/len(accs)}')\n",
    "print(f'Parameters: LR: {learning_rate} | Epochs: {params.nEpochs} | K-folds: {k_fold} | BatchSize: {params.batchSize} | Sample timesteps: {sample_timesteps}')\n",
    "\n",
    "print(f'Train subs: {train_subs} || Test subs: {test_subs}')\n",
    "\n",
    "with open('abc.txt', 'w') as abc_file:\n",
    "    for cf in cf_matrix:\n",
    "        abc_file.write(f'{cf}\\n')\n",
    "    abc_file.write(f'{accs}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
