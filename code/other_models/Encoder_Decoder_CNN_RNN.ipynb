{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import nilearn as nil\n",
    "from nilearn import image as nil_image\n",
    "import numpy as np\n",
    "\n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.autograd as dif\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.modules.utils import _triple\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# This is imported to fix any data error in a batch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom import\n",
    "from train_test_set import train_test_subs, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D_output_size(img_size, padding, kernel_size, stride):\n",
    "    # compute output shape of conv2D\n",
    "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
    "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int))\n",
    "    return outshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D CNN encoder train from scratch (no transfer learning)\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, img_x=68, img_y=49, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        self.img_x = img_x\n",
    "        self.img_y = img_y\n",
    "        inp_channels = 57\n",
    "        self.CNN_embed_dim = CNN_embed_dim\n",
    "\n",
    "        # CNN architechtures\n",
    "        self.ch1, self.ch2, self.ch3, self.ch4 = 32, 64, 128, 256\n",
    "        self.k1, self.k2, self.k3, self.k4 = (\n",
    "            5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n",
    "        self.s1, self.s2, self.s3, self.s4 = (\n",
    "            2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n",
    "        self.pd1, self.pd2, self.pd3, self.pd4 = (\n",
    "            0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
    "\n",
    "        # conv2D output shapes\n",
    "        self.conv1_outshape = conv2D_output_size(\n",
    "            (self.img_x, self.img_y), self.pd1, self.k1, self.s1)  # Conv1 output shape\n",
    "        self.conv2_outshape = conv2D_output_size(\n",
    "            self.conv1_outshape, self.pd2, self.k2, self.s2)\n",
    "        self.conv3_outshape = conv2D_output_size(\n",
    "            self.conv2_outshape, self.pd3, self.k3, self.s3)\n",
    "        self.conv4_outshape = conv2D_output_size(\n",
    "            self.conv3_outshape, self.pd4, self.k4, self.s4)\n",
    "\n",
    "        # fully connected layer hidden nodes\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=inp_channels, out_channels=self.ch1,\n",
    "                      kernel_size=self.k1, stride=self.s1, padding=self.pd1),\n",
    "            nn.BatchNorm2d(self.ch1, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch1, out_channels=self.ch2,\n",
    "                      kernel_size=self.k2, stride=self.s2, padding=self.pd2),\n",
    "            nn.BatchNorm2d(self.ch2, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch2, out_channels=self.ch3,\n",
    "                      kernel_size=self.k3, stride=self.s3, padding=self.pd3),\n",
    "            nn.BatchNorm2d(self.ch3, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch3, out_channels=self.ch4,\n",
    "                      kernel_size=self.k4, stride=self.s4, padding=self.pd4),\n",
    "            nn.BatchNorm2d(self.ch4, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.drop = nn.Dropout2d(self.drop_p)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # fully connected layer, output k classes\n",
    "        self.fc1 = nn.Linear(\n",
    "            self.ch4 * self.conv4_outshape[0] * self.conv4_outshape[1], self.fc_hidden1)\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        # output = CNN embedding latent variables\n",
    "        self.fc3 = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)\n",
    "\n",
    "    def forward(self, x_3d):\n",
    "        cnn_embed_seq = []\n",
    "        for t in range(x_3d.size(1)):\n",
    "            # CNNs\n",
    "            x = self.conv1(x_3d[:, t, :, :, :])\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv3(x)\n",
    "            x = self.conv4(x)\n",
    "            x = x.view(x.size(0), -1)           # flatten the output of conv\n",
    "\n",
    "            # FC layers\n",
    "            x = F.relu(self.fc1(x))\n",
    "            # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            x = self.fc3(x)\n",
    "            cnn_embed_seq.append(x)\n",
    "\n",
    "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
    "\n",
    "        return cnn_embed_seq\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=6):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.RNN_input_size = CNN_embed_dim\n",
    "        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n",
    "        self.h_RNN = h_RNN                 # RNN hidden nodes\n",
    "        self.h_FC_dim = h_FC_dim\n",
    "        self.drop_p = drop_p\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.LSTM = nn.LSTM(\n",
    "            input_size=self.RNN_input_size,\n",
    "            hidden_size=self.h_RNN,\n",
    "            num_layers=h_RNN_layers,\n",
    "            # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\n",
    "        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x_RNN):\n",
    "\n",
    "        self.LSTM.flatten_parameters()\n",
    "        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)\n",
    "        \"\"\" h_n shape (n_layers, batch, hidden_size), h_c shape (n_layers, batch, hidden_size) \"\"\"\n",
    "        \"\"\" None represents zero initial hidden state. RNN_out has shape=(batch, time_step, output_size) \"\"\"\n",
    "\n",
    "        # FC layers\n",
    "        # choose RNN_out at the last time step\n",
    "        x = self.fc1(RNN_out[:, -1, :])\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmriDataset(Dataset):\n",
    "\n",
    "    def __init__(self, params, data_dir='/data/fmri/data', mask_path='/data/fmri/mask/caudate._mask.nii',\n",
    "                 img_shape=(57, 68, 49, 135)):\n",
    "        self.data_dir, self.params = data_dir, params\n",
    "        self.img_timesteps = params.img_timesteps\n",
    "        self.num_classes = params.nClass\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mask_path, self.img_shape = mask_path, img_shape\n",
    "        self.samples = []\n",
    "        # Initialize the image indexes with their scores\n",
    "        self.index_data()\n",
    "        # self.mask = self.read_mask()\n",
    "        self.class_weights = self.find_weights()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path, score = self.samples[idx]\n",
    "            score = self.get_class(score)\n",
    "            img = self.read_image(img_path)\n",
    "            # img = self.apply_mask(img)\n",
    "            img = self.apply_temporal_aug(img)\n",
    "            return img, score\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def index_data(self):\n",
    "        \"\"\"\n",
    "        Stores all the image_paths with their respective scores/classes in the \n",
    "        \"\"\"\n",
    "        self.weights = {i: 0 for i in range(self.num_classes)}\n",
    "        for sub in os.listdir(self.data_dir):\n",
    "            if sub not in self.params.subs:\n",
    "                # Don't consider subjects that are not in the subs set\n",
    "                continue\n",
    "            sub_dir = os.path.join(self.data_dir, sub)\n",
    "            preproc_dir = os.path.join(sub_dir, f'{sub}.preproc')\n",
    "            for img_name in os.listdir(preproc_dir):\n",
    "                img_path = os.path.join(preproc_dir, img_name)\n",
    "                score = self.get_score(sub_dir, img_name)\n",
    "                score_class = self.get_class(score)\n",
    "                # Since we are randomly sampling 30 timesteps from each scan of 135 timesteps,\n",
    "                # I am considering the same image for \"n\" times so that we have more data to train\n",
    "                n = 5\n",
    "                for k in range(n):\n",
    "                    self.weights[score_class] += 1\n",
    "                    self.samples.append((img_path, score))\n",
    "\n",
    "    def get_class(self, score):\n",
    "        \"\"\"\n",
    "        Categorize each score into one of the five classes (bins)\n",
    "        Returns values from 0-5 (6 classes)\n",
    "        Classes: (0, 10), (10, 20), (20, 40), (40, 60), (60, 80), (80, 100)\n",
    "        \"\"\"\n",
    "        if score < 10:\n",
    "            return 0\n",
    "        elif score >= 10 and score < 20:\n",
    "            return 1\n",
    "        elif score >= 20 and score < 40:\n",
    "            return 2\n",
    "        elif score >= 40 and score < 60:\n",
    "            return 3\n",
    "        elif score >= 60 and score < 80:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "\n",
    "    def get_score(self, sub_dir, img_name):\n",
    "        score_file = '0back_VAS-f.1D' if '0back' in img_name else '2back_VAS-f.1D'\n",
    "        score_path = os.path.join(sub_dir, score_file)\n",
    "        with open(score_path, 'r') as s_f:\n",
    "            scores = [int(str(score.replace('\\n', ''))) for score in s_f]\n",
    "\n",
    "        task_num = img_name.split('.')[1]\n",
    "        score_num = int(task_num[-1:])\n",
    "        return scores[score_num]\n",
    "\n",
    "    def read_image(self, img_path):\n",
    "        nX, nY, nZ, nT = self.img_shape\n",
    "        img = nil_image.load_img(img_path)\n",
    "        img = img.get_fdata()[:nX, :nY, :nZ, :nT]\n",
    "        img = torch.tensor(img, dtype=torch.float, device=self.device)\n",
    "        img = (img - img.mean()) / img.std()\n",
    "        return img\n",
    "\n",
    "    def read_mask(self):\n",
    "        nX, nY, nZ, _ = self.img_shape\n",
    "        mask_img = nil.image.load_img(self.mask_path)\n",
    "        mask_img = mask_img.get_fdata()[:]\n",
    "        mask_img = np.asarray(mask_img)\n",
    "        dilated_mask = np.zeros((nX, nY, nZ))\n",
    "        ratio = round(mask_img.shape[2]/nZ)\n",
    "        for k in range(nZ):\n",
    "            temp = ndimage.morphology.binary_dilation(\n",
    "                mask_img[:, :, k*ratio], iterations=1) * 1\n",
    "            temp_img = Image.fromarray(np.uint8(temp*255))\n",
    "            dilated_mask[:, :, k] = np.array(temp_img.resize((nY, nX)))\n",
    "\n",
    "        dilated_mask = (dilated_mask > 64).astype(int)\n",
    "        dilated_mask = torch.tensor(\n",
    "            dilated_mask, dtype=torch.float, device=self.device)\n",
    "        return dilated_mask\n",
    "\n",
    "    def apply_mask(self, img):\n",
    "        nT = img.shape[-1]\n",
    "        for i in range(nT):\n",
    "            img[:, :, :, i] = torch.mul(img[:, :, :, i], self.mask)\n",
    "        return img\n",
    "\n",
    "    def apply_temporal_aug(self, img):\n",
    "        \"\"\"\n",
    "        Image shape: X, Y, Z, t=135\n",
    "        So, e.g: take any 30 random timesteps from the 135 available in ascending order \n",
    "        \"\"\"\n",
    "        total_timesteps = img.shape[3]\n",
    "        rand_timesteps = sorted(random.sample(\n",
    "            range(0, total_timesteps), self.img_timesteps))\n",
    "        # Move time axes to the first place followed by X, Y, Z\n",
    "        img = img.permute(3, 0, 1, 2)\n",
    "        img = torch.tensor(np.take(img.cpu().numpy(), rand_timesteps, axis=0))\n",
    "        return img\n",
    "\n",
    "    def find_weights(self):\n",
    "        weights = dict(self.weights)\n",
    "        key_max = max(weights.keys(), key=(lambda k: weights[k]))\n",
    "        max_value = weights[key_max]\n",
    "        for key in weights.keys():\n",
    "            # Add 1 to the denominator to avoid divide by zero error (in some cases)\n",
    "            weights[key] = max_value / (weights[key]+1)\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    # Function to catch errors while reading a batch of fMRI scans\n",
    "    # Removes any NoneType values from the batch to prevent errors while training\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return default_collate(batch)\n",
    "    \n",
    "\n",
    "def train_test_length(total, test_pct=0.2):\n",
    "    train_count = int((1-test_pct)*total)\n",
    "    test_count = total - train_count\n",
    "    return train_count, test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, train_loader, loss_function, optimizer, test_loader):\n",
    "    print('Training...')\n",
    "    cnn_encoder, rnn_decoder = models\n",
    "    \n",
    "    for epoch in range(params.nEpochs):\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = rnn_decoder(cnn_encoder(inputs))\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        _, _, train_acc = test(models, train_loader)\n",
    "        _, _, test_acc = test(models, test_loader)\n",
    "\n",
    "        print(f'Epoch: {epoch} | Loss: {loss} | Train Acc: {train_acc} | Test Acc: {test_acc}')\n",
    "\n",
    "    return [cnn_encoder, rnn_decoder]\n",
    "\n",
    "\n",
    "def test(models, test_loader):\n",
    "    cnn_encoder, rnn_decoder = models\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    preds = []\n",
    "    actual = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            if not data:\n",
    "                continue\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            class_outputs = rnn_decoder(cnn_encoder(inputs))\n",
    "            _, class_prediction = torch.max(class_outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (class_prediction == labels).sum().item()\n",
    "            preds.extend(list(class_prediction.to(dtype=torch.int64)))\n",
    "\n",
    "            actual.extend(list(labels.to(dtype=torch.int64)))\n",
    "\n",
    "    acc = 100*correct/total\n",
    "    # print(f'Accuracy: {acc}')\n",
    "    return preds, actual, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: LR: 0.0001 | Epochs: 10 | K-folds: 1 | BatchSize: 10 | Sample timesteps: 50\n"
     ]
    }
   ],
   "source": [
    "# Initial parameters defined\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "k_fold = 1\n",
    "params.nEpochs = 10\n",
    "accs = []\n",
    "cf_matrix = []\n",
    "learning_rate = 0.0001\n",
    "sample_timesteps = 50\n",
    "params.update({'img_timesteps': sample_timesteps})\n",
    "\n",
    "print(f'Parameters: LR: {learning_rate} | Epochs: {params.nEpochs} | K-folds: {k_fold} | BatchSize: {params.batchSize} | Sample timesteps: {sample_timesteps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For only one fold for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing subjects from the dataset\n",
    "train_subs, test_subs = train_test_subs(test_pct=0.2)\n",
    "\n",
    "# Add 'subs' key in the params to track which subjects to use to create both trainset and testset\n",
    "params.update({'subs': train_subs})\n",
    "trainset = FmriDataset(params=params)\n",
    "params.update({'subs': test_subs})\n",
    "testset = FmriDataset(params=params)\n",
    "\n",
    "# Identify the training set class weights based on their occurences in the training data\n",
    "class_weights = torch.FloatTensor(\n",
    "    [trainset.class_weights[i] for i in range(params.nClass)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/167 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 2\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [40:56<00:00, 14.71s/it]\n",
      "100%|██████████| 167/167 [40:42<00:00, 14.63s/it]\n",
      "100%|██████████| 40/40 [09:41<00:00, 14.54s/it]\n",
      "  0%|          | 0/167 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: nan | Train Acc: 45.50898203592814 | Test Acc: 56.962025316455694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [40:55<00:00, 14.70s/it]\n",
      "100%|██████████| 167/167 [40:39<00:00, 14.61s/it]\n",
      "100%|██████████| 40/40 [09:42<00:00, 14.57s/it]\n",
      "  0%|          | 0/167 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: nan | Train Acc: 45.50898203592814 | Test Acc: 56.962025316455694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 41/167 [10:04<31:01, 14.78s/it]"
     ]
    }
   ],
   "source": [
    "for k in range(k_fold):\n",
    "    # Split the train and validation sets\n",
    "    train_subs, test_subs = train_test_subs(test_pct=0.2)\n",
    "    params.update({'subs': train_subs})\n",
    "    trainset = FmriDataset(params=params)\n",
    "    params.update({'subs': test_subs})\n",
    "    testset = FmriDataset(params=params)\n",
    "    \n",
    "    # Get the class weights based on their number of instances\n",
    "    class_weights = torch.FloatTensor(\n",
    "        [trainset.class_weights[i] for i in range(params.nClass)]).to(device)\n",
    "    \n",
    "    \n",
    "    # Initialize the models\n",
    "    cnn_encoder = EncoderCNN().to(device)\n",
    "    rnn_decoder = DecoderRNN().to(device)\n",
    "    \n",
    "    # Distributed training on multiple GPUs if available\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f'Number of GPUs available: {n_gpus}')\n",
    "    if (device.type == 'cuda') and (n_gpus > 1):\n",
    "        cnn_encoder = nn.DataParallel(cnn_encoder)\n",
    "        rnn_decoder = nn.DataParallel(rnn_decoder)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    crnn_params = list(cnn_encoder.parameters()) + list(rnn_decoder.parameters())\n",
    "    optimizer = torch.optim.Adam(crnn_params, lr=learning_rate)\n",
    "\n",
    "    \n",
    "    # Prepare the train and validation loaders\n",
    "    train_loader = DataLoader(\n",
    "        trainset, batch_size=params.batchSize, shuffle=True, collate_fn=my_collate)\n",
    "    test_loader = DataLoader(\n",
    "        testset, batch_size=params.batchSize, shuffle=True, collate_fn=my_collate)\n",
    "    \n",
    "    # You can use my_collate() function inside the dataloader to check for errors while reading corrupted scans\n",
    "    models = [cnn_encoder, rnn_decoder]\n",
    "    models = train(models, train_loader, loss_function, optimizer, test_loader)\n",
    "    \n",
    "    \n",
    "    # Save the model checkpoint\n",
    "#     current_time = datetime.now()\n",
    "#     current_time = current_time.strftime(\"%m%d%Y%H_%M\")\n",
    "#     torch.save(net.state_dict(), f'{current_time}-scans-5-fold-{k}.pth')\n",
    "\n",
    "    # Test the model\n",
    "    preds, actual, acc = test(models, test_loader)\n",
    "    accs.append(acc)\n",
    "\n",
    "    # For confusion matrix\n",
    "    preds = [int(k) for k in preds]\n",
    "    actual = [int(k) for k in actual]\n",
    "\n",
    "    cf = confusion_matrix(actual, preds, labels=list(range(params.nClass)))\n",
    "    cf_matrix.append(cf)\n",
    "    with open('cf_matrices.txt', 'a') as cf_file:\n",
    "        cf_file.write(str(cf) + '\\n')\n",
    "\n",
    "\n",
    "print(cf_matrix)\n",
    "print(accs)\n",
    "print(f'Avg Accuracy: {sum(accs)/len(accs)}')\n",
    "print(f'Parameters: LR: {learning_rate} | Epochs: {params.nEpochs} | K-folds: {k_fold} | BatchSize: {params.batchSize} | Sample timesteps: {sample_timesteps}')\n",
    "\n",
    "with open('abc.txt', 'w') as abc_file:\n",
    "    for cf in cf_matrix:\n",
    "        abc_file.write(f'{cf}\\n')\n",
    "    abc_file.write(f'{accs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
